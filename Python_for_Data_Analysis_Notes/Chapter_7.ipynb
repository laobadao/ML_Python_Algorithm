{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Analysis v2 | Notes_ Chapter_7\n",
    "\n",
    " 本人以简书作者 SeanCheney 系列专题文章并结合原书为学习资源，记录个人笔记，仅作为知识记录及后期复习所用，原作者地址查看 [简书 SeanCheney](https://www.jianshu.com/u/130f76596b02)，如有错误，还望批评指教。——ZJ\n",
    "\n",
    "\n",
    ">原作者：SeanCheney | [《利用 Python 进行数据分析·第2版》第7章 数据清洗和准备](https://www.jianshu.com/p/ac7bec000dad) | 來源：简书\n",
    "\n",
    ">[Github:wesm](https://github.com/wesm/pydata-book) | [Github:中文 BrambleXu](https://github.com/BrambleXu/pydata-notebook)|\n",
    "简书:[利用   Python   进行数据分析·第2版](https://www.jianshu.com/c/52882df3377a)\n",
    "\n",
    "环境：   Python    3.6 \n",
    "\n",
    "---\n",
    "\n",
    "# Chapter 7 Data Cleaning and Preparation\n",
    "\n",
    "- 在数据分析和建模的过程中，相当多的时间要用在数据准备上：**加载、清理、转换以及重塑。**这些工作会占到分析师时间的 80% 或更多。有时，存储在文件和数据库中的数据的格式不适合某个特定的任务。\n",
    "- pandas 和内置的 Python 标准库提供了一组高级的、灵活的、快速的工具，可以让你轻松地将数据规变为想要的格式。\n",
    "- 在本章中，我会讨论**处理缺失数据、重复数据、字符串操作和其它分析数据转换的工具**。下一章，我会关注于用多种方法合并、重塑数据集。\n",
    "\n",
    "## 7.1 处理缺失数据\n",
    "\n",
    "在许多数据分析工作中，缺失数据是经常发生的。 pandas 的目标之一就是尽量轻松地处理缺失数据。例如， pandas 对象的所有描述性统计默认都不包括缺失数据。\n",
    "\n",
    "缺失数据在 pandas 中呈现的方式有些不完美，但对于大多数用户可以保证功能正常。对于数值数据， pandas 使用浮点值 NaN（Not a Number）表示缺失数据。我们称其为哨兵值，可以方便的检测出来：\n",
    "\n",
    "```\n",
    "In [1]: import numpy as np\n",
    "\n",
    "In [2]: import pandas as pd\n",
    "\n",
    "In [3]: string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])\n",
    "\n",
    "In [4]: string_data\n",
    "Out[4]:\n",
    "0     aardvark\n",
    "1    artichoke\n",
    "2          NaN\n",
    "3      avocado\n",
    "dtype: object\n",
    "\n",
    "In [5]: string_data.isnull()\n",
    "Out[5]:\n",
    "0    False\n",
    "1    False\n",
    "2     True\n",
    "3    False\n",
    "dtype: bool\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "- 在 pandas 中，我们采用了 R 语言中的惯用法，即将缺失值表示为 NA ，它表示不可用 not available。\n",
    "- 在统计应用中， NA 数据可能是不存在的数据或者虽然存在，但是没有观察到（例如，数据采集中发生了问题）。\n",
    "- 当进行数据清洗以进行分析时，最好直接对缺失数据进行分析，以判断数据采集的问题或缺失数据可能导致的偏差。\n",
    "\n",
    "Python 内置的 None 值在对象数组中也可以作为 NA: \n",
    "\n",
    "```\n",
    "In [6]: type(string_data)\n",
    "Out[6]: pandas.core.series.Series\n",
    "\n",
    "In [7]: string_data[0] = None\n",
    "\n",
    "In [8]: string_data.isnull()\n",
    "Out[8]:\n",
    "0     True\n",
    "1    False\n",
    "2     True\n",
    "3    False\n",
    "dtype: bool\n",
    "\n",
    "\n",
    "```\n",
    " pandas 项目中还在不断优化内部细节以更好处理缺失数据，像用户API功能，例如 pandas .isnull，去除了许多恼人的细节。表7-1列出了一些关于缺失数据处理的函数。\n",
    "\n",
    "![](./images/7_1.png)\n",
    "\n",
    "## 滤除缺失数据\n",
    "\n",
    "- 过滤掉缺失数据的办法有很多种。你可以通过 `pandas.isnull`或布尔索引的手工方法，但 dropna 可能会更实用一些。\n",
    "- 对于一个 Series ， **dropna 返回一个仅含非空数据和索引值的 Series **：\n",
    "\n",
    "```\n",
    "In [9]: from numpy import nan as NA\n",
    "\n",
    "In [10]: data = pd.Series([1, NA, 3.5, NA, 7])\n",
    "\n",
    "In [11]: data.dropna()\n",
    "Out[11]:\n",
    "0    1.0\n",
    "2    3.5\n",
    "4    7.0\n",
    "dtype: float64\n",
    "\n",
    "In [12]: data\n",
    "Out[12]:\n",
    "0    1.0\n",
    "1    NaN\n",
    "2    3.5\n",
    "3    NaN\n",
    "4    7.0\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "这等价于：\n",
    "\n",
    "```\n",
    "In [14]: data[data.notnull()]\n",
    "Out[14]:\n",
    "0    1.0\n",
    "2    3.5\n",
    "4    7.0\n",
    "dtype: float64\n",
    "\n",
    "```\n",
    "- 而对于 DataFrame 对象，事情就有点复杂了。你可能希望丢弃全 NA 或含有 NA 的行或列。\n",
    "- dropna 默认丢弃任何含有缺失值的行：\n",
    "\n",
    "\n",
    "```\n",
    "In [15]:  data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA],\n",
    "    ...:                          [NA, NA, NA], [NA, 6.5, 3.]], index=['one', 't\n",
    "    ...: wo', 'three', 'four'], columns=['first', 'second', 'third'])\n",
    "    ...:\n",
    "    ...:\n",
    "\n",
    "In [16]: data\n",
    "Out[16]:\n",
    "       first  second  third\n",
    "one      1.0     6.5    3.0\n",
    "two      1.0     NaN    NaN\n",
    "three    NaN     NaN    NaN\n",
    "four     NaN     6.5    3.0\n",
    "\n",
    "In [17]: cleaned  = data.dropna()\n",
    "\n",
    "In [18]: cleaned\n",
    "Out[18]:\n",
    "     first  second  third\n",
    "one    1.0     6.5    3.0\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "传入`how='all'`将只丢弃全为 NA 的那些行\n",
    "\n",
    "\n",
    "```\n",
    "In [19]: data.dropna(how='all')\n",
    "Out[19]:\n",
    "      first  second  third\n",
    "one     1.0     6.5    3.0\n",
    "two     1.0     NaN    NaN\n",
    "four    NaN     6.5    3.0\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "用这种方式丢弃列(全是 NA 的列)，只需传入`axis=1`即可：\n",
    "\n",
    "\n",
    "```\n",
    "In [20]: data['forth'] = NA\n",
    "\n",
    "In [21]: data\n",
    "Out[21]:\n",
    "       first  second  third  forth\n",
    "one      1.0     6.5    3.0    NaN\n",
    "two      1.0     NaN    NaN    NaN\n",
    "three    NaN     NaN    NaN    NaN\n",
    "four     NaN     6.5    3.0    NaN\n",
    "\n",
    "In [22]: data.dropna(axis=1, how='all')\n",
    "Out[22]:\n",
    "       first  second  third\n",
    "one      1.0     6.5    3.0\n",
    "two      1.0     NaN    NaN\n",
    "three    NaN     NaN    NaN\n",
    "four     NaN     6.5    3.0\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "另一个滤除 DataFrame 行的问题涉及时间序列数据。假设你只想留下一部分观测数据，可以用 thresh 参数实现此目的：\n",
    "\n",
    "\n",
    "```\n",
    "In [23]: df = pd.DataFrame(np.random.randn(7,3))\n",
    "\n",
    "In [24]: df.iloc[:4, 1] = NA\n",
    "\n",
    "In [25]: df\n",
    "Out[25]:\n",
    "          0         1         2\n",
    "0 -0.052880       NaN  0.192669\n",
    "1  0.440543       NaN -0.058121\n",
    "2  0.297282       NaN -0.808425\n",
    "3 -0.429874       NaN -0.965913\n",
    "4  0.132290  0.251065  0.853049\n",
    "5  1.190240 -1.118041 -0.075022\n",
    "6  0.530970  0.033641 -0.473945\n",
    "\n",
    "In [26]: df.iloc[:2, 2] = NA\n",
    "\n",
    "In [27]: df\n",
    "Out[27]:\n",
    "          0         1         2\n",
    "0 -0.052880       NaN       NaN\n",
    "1  0.440543       NaN       NaN\n",
    "2  0.297282       NaN -0.808425\n",
    "3 -0.429874       NaN -0.965913\n",
    "4  0.132290  0.251065  0.853049\n",
    "5  1.190240 -1.118041 -0.075022\n",
    "6  0.530970  0.033641 -0.473945\n",
    "\n",
    "In [28]: df.dropna()\n",
    "Out[28]:\n",
    "         0         1         2\n",
    "4  0.13229  0.251065  0.853049\n",
    "5  1.19024 -1.118041 -0.075022\n",
    "6  0.53097  0.033641 -0.473945\n",
    "\n",
    "In [29]: df.dropna(thresh=2) # 索引 0 1 行 含有 NA 的去掉了\n",
    "Out[29]:\n",
    "          0         1         2\n",
    "2  0.297282       NaN -0.808425\n",
    "3 -0.429874       NaN -0.965913\n",
    "4  0.132290  0.251065  0.853049\n",
    "5  1.190240 -1.118041 -0.075022\n",
    "6  0.530970  0.033641 -0.473945\n",
    "\n",
    "\n",
    "```\n",
    "## 填充缺失数据\n",
    "\n",
    "- 你可能不想滤除缺失数据（有可能会丢弃跟它有关的其他数据），而是希望通过其他方式填补那些“空洞”。\n",
    "- 对于大多数情况而言， fillna 方法是最主要的函数。通过一个常数调用 fillna 就会将缺失值替换为那个常数值：\n",
    "\n",
    "```\n",
    "In [30]: df.fillna(0)\n",
    "Out[30]:\n",
    "          0         1         2\n",
    "0 -0.052880  0.000000  0.000000\n",
    "1  0.440543  0.000000  0.000000\n",
    "2  0.297282  0.000000 -0.808425\n",
    "3 -0.429874  0.000000 -0.965913\n",
    "4  0.132290  0.251065  0.853049\n",
    "5  1.190240 -1.118041 -0.075022\n",
    "6  0.530970  0.033641 -0.473945\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "若是通过一个字典调用 fillna ，就可以实现对不同的列填充不同的值：\n",
    "\n",
    "```\n",
    "In [31]: df.fillna({1: 0.5, 2: 0})\n",
    "Out[31]:\n",
    "          0         1         2\n",
    "0 -0.052880  0.500000  0.000000\n",
    "1  0.440543  0.500000  0.000000\n",
    "2  0.297282  0.500000 -0.808425\n",
    "3 -0.429874  0.500000 -0.965913\n",
    "4  0.132290  0.251065  0.853049\n",
    "5  1.190240 -1.118041 -0.075022\n",
    "6  0.530970  0.033641 -0.473945\n",
    "\n",
    "```\n",
    "\n",
    "fillna 默认会返回新对象，但也可以对现有对象进行就地修改：\n",
    "\n",
    "```\n",
    "In [32]: _ = df.fillna(0, inplace= True)\n",
    "\n",
    "In [33]: df\n",
    "Out[33]:\n",
    "          0         1         2\n",
    "0 -0.052880  0.000000  0.000000\n",
    "1  0.440543  0.000000  0.000000\n",
    "2  0.297282  0.000000 -0.808425\n",
    "3 -0.429874  0.000000 -0.965913\n",
    "4  0.132290  0.251065  0.853049\n",
    "5  1.190240 -1.118041 -0.075022\n",
    "6  0.530970  0.033641 -0.473945\n",
    "\n",
    "```\n",
    "\n",
    "对 reindexing 有效的那些插值方法也可用于 fillna ：\n",
    "\n",
    "\n",
    "```\n",
    "In [34]: df = pd.DataFrame(np.random.randn(6,3))\n",
    "\n",
    "In [35]: df.iloc[2:, 1] = NA\n",
    "\n",
    "In [36]: df\n",
    "Out[36]:\n",
    "          0         1         2\n",
    "0 -0.775110  0.000504  1.445061\n",
    "1 -0.469458  0.727227 -0.166666\n",
    "2  0.019312       NaN -0.915137\n",
    "3 -1.477259       NaN  0.423064\n",
    "4  1.620944       NaN  1.165360\n",
    "5  0.388970       NaN  0.230785\n",
    "\n",
    "In [37]: df.iloc[4:, 2] = NA\n",
    "\n",
    "In [38]: df\n",
    "Out[38]:\n",
    "          0         1         2\n",
    "0 -0.775110  0.000504  1.445061\n",
    "1 -0.469458  0.727227 -0.166666\n",
    "2  0.019312       NaN -0.915137\n",
    "3 -1.477259       NaN  0.423064\n",
    "4  1.620944       NaN       NaN\n",
    "5  0.388970       NaN       NaN\n",
    "\n",
    "In [39]: df.fillna(method='ffill')\n",
    "Out[39]:\n",
    "          0         1         2\n",
    "0 -0.775110  0.000504  1.445061\n",
    "1 -0.469458  0.727227 -0.166666\n",
    "2  0.019312  0.727227 -0.915137\n",
    "3 -1.477259  0.727227  0.423064\n",
    "4  1.620944  0.727227  0.423064\n",
    "5  0.388970  0.727227  0.423064\n",
    "\n",
    "In [40]: df.fillna(method='ffill', limit=2)\n",
    "Out[40]:\n",
    "          0         1         2\n",
    "0 -0.775110  0.000504  1.445061\n",
    "1 -0.469458  0.727227 -0.166666\n",
    "2  0.019312  0.727227 -0.915137\n",
    "3 -1.477259  0.727227  0.423064\n",
    "4  1.620944       NaN  0.423064\n",
    "5  0.388970       NaN  0.423064\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "只要有些创新，你就可以利用 fillna 实现许多别的功能。比如说，你可以传入 Series 的平均值或中位数：\n",
    "\n",
    "```\n",
    "In [41]: data = pd.Series([1., NA, 3.5, NA, 7])\n",
    "\n",
    "In [42]: data.fillna(data.mean())\n",
    "Out[42]:\n",
    "0    1.000000\n",
    "1    3.833333\n",
    "2    3.500000\n",
    "3    3.833333\n",
    "4    7.000000\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "表7-2列出了 fillna 的参考。\n",
    "\n",
    "![](./images/7_2_0.png)\n",
    "![](./images/7_2.png)\n",
    "\n",
    "## 7.2 数据转换\n",
    "\n",
    "本章到目前为止介绍的都是数据的重排。另一类重要操作则是过滤、清理以及其他的转换工作。\n",
    "\n",
    "## 移除重复数据\n",
    "\n",
    "DataFrame 中出现重复行有多种原因。下面就是一个例子：\n",
    "\n",
    "```\n",
    "In [59]: data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],\n",
    "    ...:    ....:                      'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "    ...:\n",
    "\n",
    "In [60]: data\n",
    "Out[60]:\n",
    "    k1  k2\n",
    "0  one   1\n",
    "1  two   1\n",
    "2  one   2\n",
    "3  two   3\n",
    "4  one   3\n",
    "5  two   4\n",
    "6  two   4\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "DataFrame 的duplicated方法返回一个布尔型 Series ，表示各行是否是重复行（前面出现过的行）：\n",
    "\n",
    "\n",
    "```\n",
    "In [62]: data.duplicated()\n",
    "Out[62]:\n",
    "0    False\n",
    "1    False\n",
    "2    False\n",
    "3    False\n",
    "4    False\n",
    "5    False\n",
    "6     True\n",
    "dtype: bool\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "还有一个与此相关的`drop_duplicates`方法，它会返回一个 DataFrame ，重复的数组会标为 False(哪里标 False 了 )：\n",
    "\n",
    "```\n",
    "In [63]: data.drop_duplicates()\n",
    "Out[63]:\n",
    "    k1  k2\n",
    "0  one   1\n",
    "1  two   1\n",
    "2  one   2\n",
    "3  two   3\n",
    "4  one   3\n",
    "5  two   4\n",
    "\n",
    "In [64]: type(data.drop_duplicates())\n",
    "Out[64]: pandas.core.frame.DataFrame\n",
    "\n",
    "# 5  two   4\n",
    "# 6  two   4\n",
    "# 原来这两行 是相同的，所以重复 去掉最后一个\n",
    "\n",
    "```\n",
    "这两个方法默认会判断全部列，你也可以指定部分列进行重复项判断。假设我们还有一列值，且只希望根据 k1 列过滤重复项：\n",
    "\n",
    "\n",
    "```\n",
    "In [65]: data['v1'] = range(7)\n",
    "\n",
    "In [66]: data\n",
    "Out[66]:\n",
    "    k1  k2  v1\n",
    "0  one   1   0\n",
    "1  two   1   1\n",
    "2  one   2   2\n",
    "3  two   3   3\n",
    "4  one   3   4\n",
    "5  two   4   5\n",
    "6  two   4   6\n",
    "\n",
    "In [67]: data.drop_duplicates(['k1'])\n",
    "Out[67]:\n",
    "    k1  k2  v1\n",
    "0  one   1   0\n",
    "1  two   1   1\n",
    "\n",
    "# 只判断 k1 中重复的，不判断 k2, v2\n",
    "\n",
    "```\n",
    "\n",
    "`duplicated` 和 `drop_duplicates` 默认保留的是第一个出现的值组合。传入`keep='last'`则保留最后一个：\n",
    "\n",
    "```\n",
    "In [68]: data.drop_duplicates(['k1', 'k2'], keep='last')\n",
    "Out[68]:\n",
    "    k1  k2  v1\n",
    "0  one   1   0\n",
    "1  two   1   1\n",
    "2  one   2   2\n",
    "3  two   3   3\n",
    "4  one   3   4\n",
    "6  two   4   6\n",
    "\n",
    "# 5  two   4   5\n",
    "# 6  two   4   6\n",
    "# 这两行 因为只判断  'k1', 'k2' ， keep='last' 保留最后一个，所以保留 索引 6 去掉索引 5 \n",
    "```\n",
    "\n",
    "## 利用函数或映射进行数据转换\n",
    "\n",
    "对于许多数据集，你可能希望根据数组、 Series 或 DataFrame 列中的值来实现转换工作。我们来看看下面这组有关肉类的数据：\n",
    "\n",
    "\n",
    "```\n",
    "In [70]: data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',\n",
    "    ...:    ....:                               'Pastrami', 'corned beef', 'Baco\n",
    "    ...: n',\n",
    "    ...:    ....:                               'pastrami', 'honey ham', 'nova l\n",
    "    ...: ox'],\n",
    "    ...:    ....:                      'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]}\n",
    "    ...: )\n",
    "    ...:\n",
    "\n",
    "In [71]: data\n",
    "Out[71]:\n",
    "          food  ounces\n",
    "0        bacon     4.0\n",
    "1  pulled pork     3.0\n",
    "2        bacon    12.0\n",
    "3     Pastrami     6.0\n",
    "4  corned beef     7.5\n",
    "5        Bacon     8.0\n",
    "6     pastrami     3.0\n",
    "7    honey ham     5.0\n",
    "8     nova lox     6.0\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "假设你想要添加一列表示该肉类食物来源的动物类型。我们先编写一个不同肉类到动物的映射：\n",
    "\n",
    "```\n",
    "In [72]: meat_to_animal = {\n",
    "    ...:   'bacon': 'pig',\n",
    "    ...:   'pulled pork': 'pig',\n",
    "    ...:   'pastrami': 'cow',\n",
    "    ...:   'corned beef': 'cow',\n",
    "    ...:   'honey ham': 'pig',\n",
    "    ...:   'nova lox': 'salmon'\n",
    "    ...: }\n",
    "\n",
    "```\n",
    "\n",
    "- Series 的 map 方法可以接受一个函数或含有映射关系的字典型对象，但是这里有一个小问题，即有些肉类的首字母大写了，而另一些则没有。\n",
    "- 因此，我们还需要使用 Series 的 `str.lower`方法，将各个值转换为小写：\n",
    "\n",
    "\n",
    "```\n",
    "In [73]: lowercased = data['food'].str.lower()\n",
    "\n",
    "In [74]: lowercased\n",
    "Out[74]:\n",
    "0          bacon\n",
    "1    pulled pork\n",
    "2          bacon\n",
    "3       pastrami\n",
    "4    corned beef\n",
    "5          bacon\n",
    "6       pastrami\n",
    "7      honey ham\n",
    "8       nova lox\n",
    "Name: food, dtype: object\n",
    "\n",
    "# data 中添加 animal 一列，这列中的数据，是 lowercased （food 列 转化为小写后的）作为 key  # 从 meat_to_animal  这个字典中取出对应的 value\n",
    "\n",
    "In [75]: data['animal'] = lowercased.map(meat_to_animal)\n",
    "\n",
    "In [76]: data\n",
    "Out[76]:\n",
    "          food  ounces  animal\n",
    "0        bacon     4.0     pig\n",
    "1  pulled pork     3.0     pig\n",
    "2        bacon    12.0     pig\n",
    "3     Pastrami     6.0     cow\n",
    "4  corned beef     7.5     cow\n",
    "5        Bacon     8.0     pig\n",
    "6     pastrami     3.0     cow\n",
    "7    honey ham     5.0     pig\n",
    "8     nova lox     6.0  salmon\n",
    "\n",
    "\n",
    "```\n",
    "我们也可以传入一个能够完成全部这些工作的函数：\n",
    "\n",
    "\n",
    "```\n",
    "In [77]: data['food'].map(lambda x: meat_to_animal[x.lower()])\n",
    "Out[77]:\n",
    "0       pig\n",
    "1       pig\n",
    "2       pig\n",
    "3       cow\n",
    "4       cow\n",
    "5       pig\n",
    "6       cow\n",
    "7       pig\n",
    "8    salmon\n",
    "Name: food, dtype: object\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "使用 map 是一种实现元素级转换以及其他数据清理工作的便捷方式。\n",
    "\n",
    "## 替换值\n",
    "\n",
    "- 利用 fillna 方法填充缺失数据可以看做值替换的一种特殊情况。前面已经看到，map可用于修改对象的数据子集，\n",
    "- 而 replace 则提供了一种实现该功能的更简单、更灵活的方式。我们来看看下面这个 Series ：\n",
    "\n",
    "\n",
    "```\n",
    "In [78]: data = pd.Series([1., -999., 2., -999., -1000., 3.])\n",
    "\n",
    "In [79]: data\n",
    "Out[79]:\n",
    "0       1.0\n",
    "1    -999.0\n",
    "2       2.0\n",
    "3    -999.0\n",
    "4   -1000.0\n",
    "5       3.0\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "-999 这个值可能是一个表示缺失数据的标记值。要将其替换为 pandas 能够理解的 NA 值，我们可以利用 replace 来产生一个新的 Series （除非传入`inplace=True`）：\n",
    "\n",
    "```\n",
    "In [84]: data.replace(-999.0, np.nan)\n",
    "Out[84]:\n",
    "0       1.0\n",
    "1       NaN\n",
    "2       2.0\n",
    "3       NaN\n",
    "4   -1000.0\n",
    "5       3.0\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "如果你希望一次性替换多个值，可以传入一个由待替换值组成的列表以及一个替换值：：\n",
    "\n",
    "\n",
    "```\n",
    "In [85]: data.replace([-999, -1000], NA)\n",
    "Out[85]:\n",
    "0    1.0\n",
    "1    NaN\n",
    "2    2.0\n",
    "3    NaN\n",
    "4    NaN\n",
    "5    3.0\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "要让每个值有不同的替换值，可以传递一个替换列表：\n",
    "\n",
    "```\n",
    "In [86]: data.replace([-999, -1000], [np.nan, 0])\n",
    "Out[86]:\n",
    "0    1.0\n",
    "1    NaN\n",
    "2    2.0\n",
    "3    NaN\n",
    "4    0.0\n",
    "5    3.0\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "传入的参数也可以是字典：\n",
    "\n",
    "```\n",
    "In [87]: data.replace({-999: np.nan, -1000: 0})\n",
    "Out[87]:\n",
    "0    1.0\n",
    "1    NaN\n",
    "2    2.0\n",
    "3    NaN\n",
    "4    0.0\n",
    "5    3.0\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "```\n",
    "- 笔记：`data.replace` 方法与 `data.str.replace` 不同，后者做的是字符串的元素级替换。我们会在后面学习 Series 的字符串方法。\n",
    "\n",
    "## 重命名轴索引\n",
    "\n",
    "跟 Series 中的值一样，轴标签也可以通过函数或映射进行转换，从而得到一个新的不同标签的对象。轴还可以被就地修改，而无需新建一个数据结构。接下来看看下面这个简单的例子：\n",
    "\n",
    "```\n",
    "In [88]: data = pd.DataFrame(np.arange(12).reshape((3,4)), \n",
    "                index=['Ohio', 'Colorado', 'New York'],\n",
    "    ...:          columns=['one', 'two', 'three', 'four'])\n",
    "    ...:\n",
    "\n",
    "In [89]: data\n",
    "Out[89]:\n",
    "          one  two  three  four\n",
    "Ohio        0    1      2     3\n",
    "Colorado    4    5      6     7\n",
    "New York    8    9     10    11\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "跟 Series 一样，轴索引也有一个 map 方法：\n",
    "\n",
    "```\n",
    "In [90]: transform = lambda x: x[:].upper()\n",
    "\n",
    "In [91]: data.index.map(transform)\n",
    "Out[91]: Index(['OHIO', 'COLORADO', 'NEW YORK'], dtype='object')\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "你可以将其赋值给 index，这样就可以对 DataFrame 进行就地修改：\n",
    "\n",
    "```\n",
    "In [92]: data.index\n",
    "Out[92]: Index(['Ohio', 'Colorado', 'New York'], dtype='object')\n",
    "\n",
    "In [93]: data.index = data.index.map(transform)\n",
    "\n",
    "In [94]: data\n",
    "Out[94]:\n",
    "          one  two  three  four\n",
    "OHIO        0    1      2     3\n",
    "COLORADO    4    5      6     7\n",
    "NEW YORK    8    9     10    11\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "如果想要创建数据集的转换版（而不是修改原始数据），比较实用的方法是 rename：\n",
    "\n",
    "\n",
    "```\n",
    "In [95]: data.rename(index=str.title, columns=str.upper)\n",
    "Out[95]:\n",
    "          ONE  TWO  THREE  FOUR\n",
    "Ohio        0    1      2     3\n",
    "Colorado    4    5      6     7\n",
    "New York    8    9     10    11\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "特别说明一下，rename 可以结合字典型对象实现对部分轴标签的更新：\n",
    "\n",
    "```\n",
    "In [96]: data.rename(index={'OHIO': 'INDIANA'},\n",
    "    ...:    ....:             columns={'three': 'peekaboo'})\n",
    "    ...:\n",
    "Out[96]:\n",
    "          one  two  peekaboo  four\n",
    "INDIANA     0    1         2     3\n",
    "COLORADO    4    5         6     7\n",
    "NEW YORK    8    9        10    11\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "rename 可以实现复制 DataFrame 并对其索引和列标签进行赋值。如果希望就地修改某个数据集，传入`inplace=True` 即可：\n",
    "\n",
    "\n",
    "```\n",
    "In [102]: data.rename(index={'OHIO': 'INDIANA'}, inplace=True)\n",
    "\n",
    "In [103]: data\n",
    "Out[103]:\n",
    "          one  two  three  four\n",
    "INDIANA     0    1      2     3\n",
    "COLORADO    4    5      6     7\n",
    "NEW YORK    8    9     10    11\n",
    "\n",
    "\n",
    "```\n",
    "## 离散化和面元划分\n",
    "\n",
    "为了便于分析，连续数据常常被离散化或拆分为“面元”（bin）。假设有一组人员数据，而你希望将它们划分为不同的年龄组：\n",
    "\n",
    "\n",
    "```\n",
    "In [98]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "\n",
    "```\n",
    "\n",
    "接下来将这些数据划分为“18到25”、“26到35”、“35到60”以及“60以上”几个面元。要实现该功能，你需要使用 pandas 的cut函数：\n",
    "\n",
    "\n",
    "```\n",
    "In [99]: bins = [18, 25, 35, 60, 100]\n",
    "\n",
    "In [100]: cats = pd.cut(ages, bins)\n",
    "\n",
    "In [101]: cats\n",
    "Out[101]:\n",
    "[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35,\n",
    " 60], (35, 60], (25, 35]]\n",
    "Length: 12\n",
    "Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]\n",
    "\n",
    "```\n",
    "pandas 返回的是一个特殊的 Categorical 对象。结果展示了 `pandas.cut` 划分的面元。你可以将其看做一组表示面元名称的字符串。它的底层含有一个表示不同分类名称的类型数组，以及一个 `codes`属性中的年龄数据的标签：\n",
    "\n",
    "```\n",
    "In [104]: cats.codes\n",
    "Out[104]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)\n",
    "\n",
    "In [105]: cats.categories\n",
    "Out[105]:\n",
    "IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]]\n",
    "              closed='right',\n",
    "              dtype='interval[int64]')\n",
    "\n",
    "In [106]: pd.value_counts(cats)\n",
    "Out[106]:\n",
    "(18, 25]     5\n",
    "(35, 60]     3\n",
    "(25, 35]     3\n",
    "(60, 100]    1\n",
    "dtype: int64\n",
    "\n",
    "```\n",
    "`pd.value_counts(cats)`是 `pandas.cut`结果的面元计数。\n",
    "\n",
    "跟“区间”的数学符号一样，圆括号表示开端，而方括号则表示闭端（包括）。哪边是闭端可以通过`right=False`进行修改：\n",
    "\n",
    "```\n",
    "In [107]: pd.cut(ages, [18, 26, 36, 61, 100], right=False)\n",
    "Out[107]:\n",
    "[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36,\n",
    " 61), [36, 61), [26, 36)]\n",
    "Length: 12\n",
    "Categories (4, interval[int64]): [[18, 26) < [26, 36) < [36, 61) < [61, 100)]\n",
    "\n",
    "```\n",
    "你可以通过传递一个列表或数组到 labels，设置自己的面元名称：\n",
    "\n",
    "```\n",
    "In [108]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']\n",
    "\n",
    "In [109]: pd.cut(ages, bins, labels=group_names)\n",
    "Out[109]:\n",
    "[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, Mid\n",
    "dleAged, YoungAdult]\n",
    "Length: 12\n",
    "Categories (4, object): [Youth < YoungAdult < MiddleAged < Senior]\n",
    "\n",
    "```\n",
    "如果向cut传入的是面元的数量而不是确切的面元边界，则它会根据数据的最小值和最大值计算等长面元。下面这个例子中，我们将一些均匀分布的数据分成四组：\n",
    "\n",
    "```\n",
    "In [110]: data = np.random.rand(20)\n",
    "\n",
    "In [111]: pd.cut(data, 4, precision=2)\n",
    "Out[111]:\n",
    "[(0.73, 0.96], (0.028, 0.26], (0.26, 0.49], (0.49, 0.73], (0.73, 0.96], ..., (0.2\n",
    "6, 0.49], (0.26, 0.49], (0.26, 0.49], (0.49, 0.73], (0.73, 0.96]]\n",
    "Length: 20\n",
    "Categories (4, interval[float64]): [(0.028, 0.26] < (0.26, 0.49] < (0.49, 0.73] <\n",
    " (0.73, 0.96]]\n",
    "\n",
    "```\n",
    "\n",
    "选项`precision=2`，限定小数只有两位。\n",
    "\n",
    "- qcut 是一个非常类似于 cut 的函数，它可以根据样本分位数对数据进行面元划分。\n",
    "- 根据数据的分布情况，cut 可能无法使各个面元中含有相同数量的数据点。而 qcut 由于使用的是样本分位数，因此可以得到大小基本相等的面元：\n",
    "\n",
    "```\n",
    "In [112]: data = np.random.randn(1000)\n",
    "\n",
    "In [113]: cats = pd.qcut(data, 4)\n",
    "\n",
    "In [114]: cats\n",
    "Out[114]:\n",
    "[(0.625, 2.842], (0.625, 2.842], (-3.054, -0.78], (0.625, 2.842], (-3.054, -0.78]\n",
    ", ..., (-3.054, -0.78], (-3.054, -0.78], (-3.054, -0.78], (0.625, 2.842], (-3.054\n",
    ", -0.78]]\n",
    "Length: 1000\n",
    "Categories (4, interval[float64]): [(-3.054, -0.78] < (-0.78, -0.0355] < (-0.0355\n",
    ", 0.625] <\n",
    "                                    (0.625, 2.842]]\n",
    "\n",
    "In [115]: pd.value_counts(cats)\n",
    "Out[115]:\n",
    "(0.625, 2.842]      250\n",
    "(-0.0355, 0.625]    250\n",
    "(-0.78, -0.0355]    250\n",
    "(-3.054, -0.78]     250\n",
    "dtype: int64\n",
    "\n",
    "```\n",
    "\n",
    "与 cut 类似，你也可以传递自定义的分位数（0到1之间的数值，包含端点）：\n",
    "\n",
    "```\n",
    "In [117]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])\n",
    "Out[117]:\n",
    "[(-0.0355, 1.241], (1.241, 2.842], (-1.359, -0.0355], (1.241, 2.842], (-3.054, -1\n",
    ".359], ..., (-3.054, -1.359], (-3.054, -1.359], (-1.359, -0.0355], (-0.0355, 1.24\n",
    "1], (-1.359, -0.0355]]\n",
    "Length: 1000\n",
    "Categories (4, interval[float64]): [(-3.054, -1.359] < (-1.359, -0.0355] < (-0.03\n",
    "55, 1.241] <\n",
    "                                    (1.241, 2.842]\n",
    "\n",
    "```\n",
    "\n",
    "- 本章稍后在讲解聚合和分组运算时会再次用到 cut 和 qcut，因为这两个离散化函数对分位和分组分析非常重要。\n",
    "\n",
    "## 检测和过滤异常值\n",
    "\n",
    "过滤或变换异常值（outlier）在很大程度上就是运用数组运算。来看一个含有正态分布数据的 DataFrame: \n",
    "\n",
    "```\n",
    "In [118]: data = pd.DataFrame(np.random.randn(1000, 4))\n",
    "\n",
    "In [119]: data.describe()\n",
    "Out[119]:\n",
    "                 0            1            2            3\n",
    "count  1000.000000  1000.000000  1000.000000  1000.000000\n",
    "mean     -0.024284     0.000331    -0.033790    -0.005900\n",
    "std       1.002445     1.030087     1.010808     1.027110\n",
    "min      -3.253857    -3.951871    -2.916422    -3.060088\n",
    "25%      -0.705493    -0.681622    -0.707278    -0.682978\n",
    "50%      -0.006853     0.011623    -0.016127     0.009688\n",
    "75%       0.643390     0.688746     0.671249     0.653106\n",
    "max       3.096767     3.574762     3.291177     3.513211\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "假设你想要找出某列中绝对值大小超过 3 的值：\n",
    "\n",
    "\n",
    "```\n",
    "In [121]: col = data[2]\n",
    "\n",
    "In [122]: col[np.abs(col) > 3]\n",
    "Out[122]:\n",
    "583    3.291177\n",
    "Name: 2, dtype: float64\n",
    "\n",
    "\n",
    "```\n",
    "要选出全部含有“超过 3 或 －3的值”的行，你可以在布尔型 DataFrame 中使用any方法：\n",
    "\n",
    "\n",
    "```\n",
    "In [123]: data[(np.abs(data) > 3).any(1)]\n",
    "Out[123]:\n",
    "            0         1         2         3\n",
    "583 -0.328060  0.512835  3.291177  0.649597\n",
    "653 -0.388316  3.574762 -0.379249 -0.429550\n",
    "706 -1.049828  0.397178 -0.011549 -3.060088\n",
    "727 -3.082909 -0.310682  0.375530  0.242903\n",
    "802 -3.253857  0.940367  0.040317  2.556581\n",
    "856  0.779090 -3.239075  0.596105 -0.218275\n",
    "901 -0.116137 -0.810662  0.290593  3.513211\n",
    "903  0.125236 -3.951871 -0.210753 -1.271565\n",
    "911  3.096767  1.309234  2.114700 -1.847553\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "根据这些条件，就可以对值进行设置。下面的代码可以将值限制在区间 －3 到 3 以内：\n",
    "\n",
    "\n",
    "```\n",
    "In [127]: data[(np.abs(data) > 3)] = np.sign(data) * 3\n",
    "\n",
    "In [128]: data.describe()\n",
    "Out[128]:\n",
    "                 0            1            2            3\n",
    "count  1000.000000  1000.000000  1000.000000  1000.000000\n",
    "mean     -0.024044     0.000947    -0.034082    -0.006353\n",
    "std       1.001111     1.024294     1.009890     1.025300\n",
    "min      -3.000000    -3.000000    -2.916422    -3.000000\n",
    "25%      -0.705493    -0.681622    -0.707278    -0.682978\n",
    "50%      -0.006853     0.011623    -0.016127     0.009688\n",
    "75%       0.643390     0.688746     0.671249     0.653106\n",
    "max       3.000000     3.000000     3.000000     3.000000\n",
    "\n",
    "\n",
    "```\n",
    "根据数据的值是正还是负，`np.sign(data)`可以生成 1 和 -1：\n",
    "\n",
    "\n",
    "```\n",
    "In [129]: np.sign(data).head()\n",
    "Out[129]:\n",
    "     0    1    2    3\n",
    "0  1.0  1.0 -1.0 -1.0\n",
    "1  1.0  1.0  1.0 -1.0\n",
    "2 -1.0 -1.0 -1.0 -1.0\n",
    "3  1.0 -1.0  1.0  1.0\n",
    "4 -1.0 -1.0 -1.0 -1.0\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## 排列和随机采样\n",
    "\n",
    "利用`numpy.random.permutation`函数可以轻松实现对 Series 或 DataFrame 的列的排列工作（permuting，随机重排序）。通过需要排列的轴的长度调用`permutation`，可产生一个表示新顺序的整数数组：\n",
    "\n",
    "\n",
    "```\n",
    "In [130]: df = pd.DataFrame(np.arange(5*4).reshape((5, 4)))\n",
    "\n",
    "In [131]: df\n",
    "Out[131]:\n",
    "    0   1   2   3\n",
    "0   0   1   2   3\n",
    "1   4   5   6   7\n",
    "2   8   9  10  11\n",
    "3  12  13  14  15\n",
    "4  16  17  18  19\n",
    "\n",
    "In [132]: sampler = np.random.permutation(5)\n",
    "\n",
    "In [133]: sampler\n",
    "Out[133]: array([2, 0, 4, 1, 3])\n",
    "\n",
    "In [134]: sampler1 = np.random.permutation(5)\n",
    "\n",
    "In [135]: sampler1\n",
    "Out[135]: array([0, 1, 3, 2, 4])\n",
    "\n",
    "\n",
    "```\n",
    "然后就可以在基于`iloc`的索引操作或`take`函数中使用该数组了：\n",
    "\n",
    "```\n",
    "In [136]: df\n",
    "Out[136]:\n",
    "    0   1   2   3\n",
    "0   0   1   2   3\n",
    "1   4   5   6   7\n",
    "2   8   9  10  11\n",
    "3  12  13  14  15\n",
    "4  16  17  18  19\n",
    "\n",
    "In [138]: df.take(sampler)\n",
    "Out[138]:\n",
    "    0   1   2   3\n",
    "2   8   9  10  11\n",
    "0   0   1   2   3\n",
    "4  16  17  18  19\n",
    "1   4   5   6   7\n",
    "3  12  13  14  15\n",
    "\n",
    "In [139]: df.take(sampler1)\n",
    "Out[139]:\n",
    "    0   1   2   3\n",
    "0   0   1   2   3\n",
    "1   4   5   6   7\n",
    "3  12  13  14  15\n",
    "2   8   9  10  11\n",
    "\n",
    "\n",
    "```\n",
    "如果不想用替换的方式选取随机子集，可以在 Series 和 DataFrame 上使用 sample 方法：\n",
    "\n",
    "\n",
    "```\n",
    "In [140]: df.sample(n=3)\n",
    "Out[140]:\n",
    "   0  1   2   3\n",
    "1  4  5   6   7\n",
    "0  0  1   2   3\n",
    "2  8  9  10  11\n",
    "\n",
    "\n",
    "```\n",
    "要通过替换的方式产生样本（允许重复选择），可以传递 `replace =True`到sample：\n",
    "\n",
    "\n",
    "```\n",
    "In [141]: choices = pd.Series([5, 7, -1, 6, 4])\n",
    "\n",
    "In [142]: draws = choices.sample(n=10, replace=True)\n",
    "\n",
    "In [143]: draws\n",
    "Out[143]:\n",
    "2   -1\n",
    "1    7\n",
    "0    5\n",
    "0    5\n",
    "4    4\n",
    "3    6\n",
    "0    5\n",
    "2   -1\n",
    "0    5\n",
    "4    4\n",
    "dtype: int64\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## 计算指标/哑变量\n",
    "\n",
    "- 另一种常用于统计建模或机器学习的转换方式是：将分类变量（categorical variable）转换为“哑变量”或“指标矩阵”。\n",
    "\n",
    "- 如果 DataFrame 的某一列中含有 k 个不同的值，则可以派生出一个 k 列矩阵或 DataFrame （其值全为 1 和 0）。 pandas 有一个 `get_dummies` 函数可以实现该功能（其实自己动手做一个也不难）。使用之前的一个 DataFrame 例子：\n",
    "\n",
    "\n",
    "```\n",
    "In [144]: df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n",
    "     ...:    .....:                    'data1': range(6)})\n",
    "     ...:\n",
    "\n",
    "In [145]: pd.get_dummies(df['key'])\n",
    "Out[145]:\n",
    "   a  b  c\n",
    "0  0  1  0\n",
    "1  0  1  0\n",
    "2  1  0  0\n",
    "3  0  0  1\n",
    "4  1  0  0\n",
    "5  0  1  0\n",
    "\n",
    "In [146]: df\n",
    "Out[146]:\n",
    "   data1 key\n",
    "0      0   b\n",
    "1      1   b\n",
    "2      2   a\n",
    "3      3   c\n",
    "4      4   a\n",
    "5      5   b\n",
    "\n",
    "\n",
    "```\n",
    "有时候，你可能想给指标 DataFrame 的列加上一个前缀，以便能够跟其他数据进行合并。 `get_dummies` 的`prefix`参数可以实现该功能：\n",
    "\n",
    "```\n",
    "In [147]: dummies = pd.get_dummies(df['key'], prefix='key')\n",
    "\n",
    "In [148]: df_with_dummy = df[['data1']].join(dummies)\n",
    "\n",
    "In [149]: df_with_dummy\n",
    "Out[149]:\n",
    "   data1  key_a  key_b  key_c\n",
    "0      0      0      1      0\n",
    "1      1      0      1      0\n",
    "2      2      1      0      0\n",
    "3      3      0      0      1\n",
    "4      4      1      0      0\n",
    "5      5      0      1      0\n",
    "\n",
    "\n",
    "In [151]: df\n",
    "Out[151]:\n",
    "   data1 key\n",
    "0      0   b\n",
    "1      1   b\n",
    "2      2   a\n",
    "3      3   c\n",
    "4      4   a\n",
    "5      5   b\n",
    "\n",
    "In [152]: df_with_dummy1 = df[['key']].join(dummies)\n",
    "\n",
    "In [153]: df_with_dummy1\n",
    "Out[153]:\n",
    "  key  key_a  key_b  key_c\n",
    "0   b      0      1      0\n",
    "1   b      0      1      0\n",
    "2   a      1      0      0\n",
    "3   c      0      0      1\n",
    "4   a      1      0      0\n",
    "5   b      0      1      0\n",
    "\n",
    "In [154]: df_with_dummy2 = df[['data1']].join(df[['key']].join(dummies))\n",
    "\n",
    "In [155]: df_with_dummy2\n",
    "Out[155]:\n",
    "   data1 key  key_a  key_b  key_c\n",
    "0      0   b      0      1      0\n",
    "1      1   b      0      1      0\n",
    "2      2   a      1      0      0\n",
    "3      3   c      0      0      1\n",
    "4      4   a      1      0      0\n",
    "5      5   b      0      1      0\n",
    "\n",
    "\n",
    "```\n",
    "如果 DataFrame 中的某行同属于多个分类，则事情就会有点复杂。看一下 MovieLens 1M 数据集，14章会更深入地研究它：\n",
    "\n",
    "\n",
    "```\n",
    "In [156]: mnames = ['movie_id', 'title', 'genres']\n",
    "     ...:\n",
    "\n",
    "In [158]: movies[:10]\n",
    "Out[158]:\n",
    "   movie_id                               title                        genres\n",
    "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
    "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
    "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
    "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
    "4         5  Father of the Bride Part II (1995)                        Comedy\n",
    "5         6                         Heat (1995)         Action|Crime|Thriller\n",
    "6         7                      Sabrina (1995)                Comedy|Romance\n",
    "7         8                 Tom and Huck (1995)          Adventure|Children's\n",
    "8         9                 Sudden Death (1995)                        Action\n",
    "9        10                    GoldenEye (1995)     Action|Adventure|Thriller\n",
    "\n",
    "\n",
    "```\n",
    "要为每个 genre 添加指标变量就需要做一些数据规整操作。首先，我们从数据集中抽取出不同的genre 值：\n",
    "\n",
    "\n",
    "```\n",
    "In [159]: all_genres = []\n",
    "\n",
    "In [160]: for x in movies.genres:\n",
    "     ...:     all_genres.extend(x.split('|')) # x.split('|') 对 Action|Adventure|Thriller 切割分割\n",
    "     ...:\n",
    "\n",
    "In [161]: genres = pd.unique(all_genres)\n",
    "\n",
    "In [162]: genres\n",
    "Out[162]:\n",
    "array(['Animation', \"Children's\", 'Comedy', 'Adventure', 'Fantasy',\n",
    "       'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror',\n",
    "       'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir',\n",
    "       'Western'], dtype=object)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "构建指标 DataFrame 的方法之一是从一个全零 DataFrame 开始：\n",
    "\n",
    "```\n",
    "In [164]: zero_matrix = np.zeros((len(movies), len(genres)))\n",
    "\n",
    "In [165]: dummies = pd.DataFrame(zero_matrix, columns=genres)\n",
    "\n",
    "In [166]: zero_matrix\n",
    "Out[166]:\n",
    "array([[0., 0., 0., ..., 0., 0., 0.],\n",
    "       [0., 0., 0., ..., 0., 0., 0.],\n",
    "       [0., 0., 0., ..., 0., 0., 0.],\n",
    "       ...,\n",
    "       [0., 0., 0., ..., 0., 0., 0.],\n",
    "       [0., 0., 0., ..., 0., 0., 0.],\n",
    "       [0., 0., 0., ..., 0., 0., 0.]])\n",
    "\n",
    "In [167]: dummies\n",
    "Out[167]:\n",
    "      Animation  Children's  Comedy  Adventure  Fantasy  Romance  Drama  \\\n",
    "0           0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "1           0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "2           0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3           0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "4           0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "5           0.0         0.0     0.0        0.0      0.0      0.0    \n",
    "\n",
    "...         ...         ...     ...        ...      ...      ...    ...\n",
    "\n",
    "3873        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3874        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3875        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3876        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3877        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3878        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3879        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3880        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3881        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "3882        0.0         0.0     0.0        0.0      0.0      0.0    0.0\n",
    "\n",
    "      Action  Crime  Thriller  Horror  Sci-Fi  Documentary  War  Musical  \\\n",
    "0        0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "1        0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "2        0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3        0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "4        0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "5        0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "\n",
    "...      ...    ...       ...     ...     ...          ...  ...      ...\n",
    "\n",
    "3874     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3875     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3876     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3877     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3878     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3879     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3880     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3881     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "3882     0.0    0.0       0.0     0.0     0.0          0.0  0.0      0.0\n",
    "\n",
    "      Mystery  Film-Noir  Western\n",
    "0         0.0        0.0      0.0\n",
    "1         0.0        0.0      0.0\n",
    "2         0.0        0.0      0.0\n",
    "3         0.0        0.0      0.0\n",
    "4         0.0        0.0      0.0\n",
    "5         0.0        0.0      0.0\n",
    "6         0.0        0.0      0.0\n",
    "7         0.0        0.0      0.0\n",
    "8         0.0        0.0      0.0\n",
    "9         0.0        0.0      0.0\n",
    "\n",
    "...       ...        ...      ...\n",
    "\n",
    "3873      0.0        0.0      0.0\n",
    "3874      0.0        0.0      0.0\n",
    "3875      0.0        0.0      0.0\n",
    "3876      0.0        0.0      0.0\n",
    "3877      0.0        0.0      0.0\n",
    "3878      0.0        0.0      0.0\n",
    "3879      0.0        0.0      0.0\n",
    "3880      0.0        0.0      0.0\n",
    "3881      0.0        0.0      0.0\n",
    "3882      0.0        0.0      0.0\n",
    "\n",
    "[3883 rows x 18 columns]\n",
    "\n",
    "\n",
    "```\n",
    "现在，迭代每一部电影，并将 dummies 各行的条目设为1。要这么做，我们使用`dummies.columns`来计算每个类型的列索引：\n",
    "\n",
    "```\n",
    "In [168]: gen = movies.genres[0]\n",
    "\n",
    "In [169]: gen.split('|')\n",
    "Out[169]: ['Animation', \"Children's\", 'Comedy']\n",
    "\n",
    "In [170]: dummies.columns.get_indexer(gen.split('|'))\n",
    "Out[170]: array([0, 1, 2], dtype=int64)\n",
    "\n",
    "\n",
    "```\n",
    "然后，根据索引，使用`.iloc`设定值：\n",
    "\n",
    "\n",
    "```\n",
    "In [171]: for i, gen in enumerate(movies.genres):\n",
    "     ...:     indices = dummies.columns.get_indexer(gen.split('|'))\n",
    "     ...:     dummies.iloc[i, indices] = 1\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "然后，和以前一样，再将其与`movies`合并起来：\n",
    "\n",
    "```\n",
    "In [173]: movies_windic = movies.join(dummies.add_prefix('Genre_'))\n",
    "\n",
    "In [174]: movies_windic.iloc[0]\n",
    "Out[174]:\n",
    "movie_id                                       1\n",
    "title                           Toy Story (1995)\n",
    "genres               Animation|Children's|Comedy\n",
    "Genre_Animation                                1\n",
    "Genre_Children's                               1\n",
    "Genre_Comedy                                   1\n",
    "Genre_Adventure                                0\n",
    "Genre_Fantasy                                  0\n",
    "Genre_Romance                                  0\n",
    "Genre_Drama                                    0\n",
    "Genre_Action                                   0\n",
    "Genre_Crime                                    0\n",
    "Genre_Thriller                                 0\n",
    "Genre_Horror                                   0\n",
    "Genre_Sci-Fi                                   0\n",
    "Genre_Documentary                              0\n",
    "Genre_War                                      0\n",
    "Genre_Musical                                  0\n",
    "Genre_Mystery                                  0\n",
    "Genre_Film-Noir                                0\n",
    "Genre_Western                                  0\n",
    "Name: 0, dtype: object\n",
    "\n",
    "\n",
    "```\n",
    "- 笔记：对于很大的数据，用这种方式构建多成员指标变量就会变得非常慢。最好使用更低级的函数，将其写入 NumPy 数组，然后结果包装在 DataFrame 中。\n",
    "\n",
    "- 一个对统计应用有用的秘诀是：结合 `get_dummies` 和诸如`cut`之类的离散化函数：\n",
    "\n",
    "```\n",
    "In [179]: np.random.seed(12345)\n",
    "\n",
    "In [180]: values = np.random.rand(10)\n",
    "\n",
    "In [181]: values\n",
    "Out[181]:\n",
    "array([0.92961609, 0.31637555, 0.18391881, 0.20456028, 0.56772503,\n",
    "       0.5955447 , 0.96451452, 0.6531771 , 0.74890664, 0.65356987])\n",
    "\n",
    "In [182]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "In [183]: pd.get_dummies(pd.cut(values, bins))\n",
    "Out[183]:\n",
    "   (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]\n",
    "0           0           0           0           0           1\n",
    "1           0           1           0           0           0\n",
    "2           1           0           0           0           0\n",
    "3           0           1           0           0           0\n",
    "4           0           0           1           0           0\n",
    "5           0           0           1           0           0\n",
    "6           0           0           0           0           1\n",
    "7           0           0           0           1           0\n",
    "8           0           0           0           1           0\n",
    "9           0           0           0           1           0\n",
    "\n",
    "```\n",
    "我们用`numpy.random.seed`，使这个例子具有确定性。本书后面会介绍 `pandas.get_dummies` 。\n",
    "\n",
    "## 7.3 字符串操作\n",
    "\n",
    "- Python 能够成为流行的数据处理语言，部分原因是其简单易用的字符串和文本处理功能。大部分文本运算都直接做成了字符串对象的内置方法。\n",
    "- 对于更为复杂的模式匹配和文本操作，则可能需要用到正则表达式。 pandas 对此进行了加强，它使你能够对整组数据应用字符串表达式和正则表达式，而且能处理烦人的缺失数据。\n",
    "\n",
    "## 字符串对象方法\n",
    "\n",
    "对于许多字符串处理和脚本应用，内置的字符串方法已经能够满足要求了。例如，以逗号分隔的字符串可以用`split`拆分成数段：\n",
    "\n",
    "```\n",
    "In [184]: val = 'a,b,  guido'\n",
    "\n",
    "In [185]: val.split(',')\n",
    "Out[185]: ['a', 'b', '  guido']\n",
    "\n",
    "```\n",
    "\n",
    "`split`常常与 `strip`一起使用，以去除空白符（包括换行符）：\n",
    "\n",
    "```\n",
    "n [187]: pieces = [x.strip() for x in val.split(',')]\n",
    "\n",
    "In [188]: pieces\n",
    "Out[188]: ['a', 'b', 'guido']\n",
    "\n",
    "\n",
    "```\n",
    "利用加法，可以将这些子字符串以双冒号分隔符的形式连接起来：\n",
    "\n",
    "\n",
    "```\n",
    "In [189]: first, second, third = pieces\n",
    "\n",
    "In [190]: first + '::' + second + '::' + third\n",
    "Out[190]: 'a::b::guido'\n",
    "\n",
    "\n",
    "```\n",
    "但这种方式并不是很实用。一种更快更符合 Python 风格的方式是，向字符串\"::\"的join方法传入一个列表或元组：\n",
    "\n",
    "```\n",
    "In [191]: '::'.join(pieces)\n",
    "Out[191]: 'a::b::guido'\n",
    "\n",
    "```\n",
    "\n",
    "其它方法关注的是子串定位。检测子串的最佳方式是利用 Python 的 in 关键字，还可以使用`index`和`find`：\n",
    "\n",
    "```\n",
    "In [192]: 'guido' in val\n",
    "Out[192]: True\n",
    "\n",
    "In [193]: val.index(',')\n",
    "Out[193]: 1\n",
    "\n",
    "In [194]: val.find(':')\n",
    "Out[194]: -1\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "注意 find 和 index 的区别：如果找不到字符串，index 将会引发一个异常（而不是返回－1）：\n",
    "\n",
    "```\n",
    "In [195]: val.index(':')\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-195-2c016e7367ac> in <module>()\n",
    "----> 1 val.index(':')\n",
    "\n",
    "ValueError: substring not found\n",
    "\n",
    "\n",
    "```\n",
    "与此相关， count 可以返回指定子串的出现次数：\n",
    "\n",
    "```\n",
    "In [196]: val.count(',')\n",
    "Out[196]: 2\n",
    "\n",
    "```\n",
    "\n",
    "replace 用于将指定模式替换为另一个模式。通过传入空字符串，它也常常用于删除模式：\n",
    "\n",
    "```\n",
    "In [197]: val.replace(',', '::')\n",
    "Out[197]: 'a::b::  guido'\n",
    "\n",
    "In [198]: val.replace(',','')\n",
    "Out[198]: 'ab  guido'\n",
    "\n",
    "\n",
    "```\n",
    "表7-3 列出了 Python 内置的字符串方法。\n",
    "\n",
    "这些运算大部分都能使用正则表达式实现（马上就会看到）。\n",
    "\n",
    "![](./images/7_3.png)\n",
    "![](./images/7_3_1.png)\n",
    "\n",
    "casefold 将字符转换为小写，并将任何特定区域的变量字符组合转换成一个通用的可比较形式。\n",
    "\n",
    "## 正则表达式\n",
    "\n",
    "正则表达式提供了一种灵活的在文本中搜索或匹配（通常比前者复杂）字符串模式的方式。正则表达式，常称作 regex ，是根据正则表达式语言编写的字符串。 Python 内置的 re 模块负责对字符串应用正则表达式。我将通过一些例子说明其使用方法。\n",
    "\n",
    "笔记：正则表达式的编写技巧可以自成一章，超出了本书的范围。从网上和其它书可以找到许多非常不错的教程和参考资料。\n",
    "\n",
    "re 模块的函数可以分为三个大类：模式匹配、替换以及拆分。当然，它们之间是相辅相成的。一个 regex 描述了需要在文本中定位的一个模式，它可以用于许多目的。我们先来看一个简单的例子：假设我想要拆分一个字符串，分隔符为数量不定的一组空白符（制表符、空格、换行符等）。描述一个或多个空白符的 regex 是 \\s+：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "调用 `re.split('\\s+',text)`时，正则表达式会先被编译，然后再在`text`上调用其`split`方法。你可以用`re.compile`自己编译 regex 以得到一个可重用的 regex 对象：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "如果只希望得到匹配 regex 的所有模式，则可以使用`findall`方法：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "笔记：如果想避免正则表达式中不需要的转义（\\），则可以使用原始字符串字面量如r'C:\\x'（也可以编写其等价式'C:\\x'）。\n",
    "\n",
    "如果打算对许多字符串应用同一条正则表达式，强烈建议通过re.compile创建 regex 对象。这样将可以节省大量的CPU时间。\n",
    "\n",
    "match 和 search 跟 findall 功能类似。findall返回的是字符串中所有的匹配项，而 search 则只返回第一个匹配项。match更加严格，它只匹配字符串的首部。来看一个小例子，假设我们有一段文本以及一条能够识别大部分电子邮件地址的正则表达式：\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "对 text 使用 findall 将得到一组电子邮件地址：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "search 返回的是文本中第一个电子邮件地址（以特殊的匹配项对象形式返回）。对于上面那个 regex ，匹配项对象只能告诉我们模式在原字符串中的起始和结束位置：\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    " `regex .match`则将返回 None ，因为它只匹配出现在字符串开头的模式：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "相关的，sub方法可以将匹配到的模式替换为指定字符串，并返回所得到的新字符串：\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "假设你不仅想要找出电子邮件地址，还想将各个地址分成3个部分：用户名、域名以及域后缀。要实现此功能，只需将待分段的模式的各部分用圆括号包起来即可：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "由这种修改过的正则表达式所产生的匹配项对象，可以通过其 groups 方法返回一个由模式各段组成的元组：\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "对于带有分组功能的模式，findall会返回一个元组列表：\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "sub还能通过诸如\\1、\\2之类的特殊符号访问各匹配项中的分组。符号\\1对应第一个匹配的组，\\2对应第二个匹配的组，以此类推：\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    " Python 中还有许多的正则表达式，但大部分都超出了本书的范围。表7-4是一个简要概括。\n",
    "\n",
    "![](./images/7_4.png)\n",
    "\n",
    "##  pandas 的矢量化字符串函数\n",
    "\n",
    "清理待分析的散乱数据时，常常需要做一些字符串规整化工作。更为复杂的情况是，含有字符串的列有时还含有缺失数据：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "通过data.map，所有字符串和正则表达式方法都能被应用于（传入lambda表达式或其他函数）各个值，但是如果存在 NA （null）就会报错。为了解决这个问题， Series 有一些能够跳过 NA 值的面向数组方法，进行字符串操作。通过 Series 的 str 属性即可访问这些方法。例如，我们可以通过 str .contains检查各个电子邮件地址是否含有\"gmail\"：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "也可以使用正则表达式，还可以加上任意re选项（如IGNORECASE）：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "有两个办法可以实现矢量化的元素获取操作：要么使用 str .get，要么在 str 属性上使用索引：\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "要访问嵌入列表中的元素，我们可以传递索引到这两个函数中：\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "你可以利用这种方法对字符串进行截取：\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "表7-5介绍了更多的 pandas 字符串方法。\n",
    "\n",
    "\n",
    "\n",
    "![](./images/7_5.png)\n",
    "\n",
    "\n",
    "## 7.4 总结\n",
    "\n",
    "高效的数据准备可以让你将更多的时间用于数据分析，花较少的时间用于准备工作，这样就可以极大地提高生产力。我们在本章中学习了许多工具，但覆盖并不全面。下一章，我们会学习 pandas 的聚合与分组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
